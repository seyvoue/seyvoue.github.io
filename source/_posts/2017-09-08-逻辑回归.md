---
title: '@逻辑回归'
comments: true
categories: 机器学习
tags: ml
mathjax: true
abbrlink: 309c0b9e
date: 2017-09-08 09:05:20
updated: 2018-02-01 18:10:14
keywords:
description:
---

> overview: logistic regression, binary classification, decision boundary, sigmoid function(logistic function), multiclass classification.

<!--more-->

## 二分类问题

### 建模
#### 非向量形式
**Hypothesis**

$$
\left\{
\begin{aligned}
h_{\theta}(x) &= g({\theta}^Tx) \\
z &= {\theta}^Tx \\
g(z) &= \frac{1}{1+e^{-z}}
\end{aligned}
\right.
\tag{1-1}
$$

*$g(z)$ 的函数图像如下：*
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-05-logistic-regression.png)

其中：

- $g(z)$ 亦称为 sigmoid function(logistic function)，其值域约束在[0,1]之间。
- $h_\theta(x)=P(y=1|x;\theta)$
- $P(y=1|x;\theta)+P(y=0|x;\theta)=1$

**Cost Function**

$$
\begin{aligned}
J(\theta) 
&= \frac{1}{m} \sum_{i=1}^mCost(h_{\theta}(x^{(i)},y^{(i)}) \\
&= - \frac{1}{m} \sum_{i=1}^m [y^{(i)} \log (h_{\theta}(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))]
\end{aligned}
\tag{1-2}
$$

$$
\left\{
\begin{aligned}
Cost(h_{\theta}(x),y)&=-\log(h_{\theta}(x)) \quad &if \; y=1 \\
Cost(h_{\theta}(x),y)&=-\log(1-h_{\theta}(x)) \quad &if \; y=0
\end{aligned}
\right.
$$

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-011740.jpg)


**Gradient Descent**
$$
\begin{align*}
& Repeat \; \lbrace \quad for \; j:=0..n \newline 
& \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline & \rbrace\end{align*}
$$

$\Leftrightarrow$

$$
\begin{align*}
& Repeat \; \lbrace \quad for \; j:=0..n \newline
& \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline & \rbrace
\end{align*}
\tag{1-3}\label{1-3}
$$


**式$\eqref{1-3}$的推导过程如下：**
$$
\begin{align*}\sigma(x)'&=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)=\sigma(x)(1 - \sigma(x))\end{align*}
$$

$$
\begin{align*}\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j\end{align*}
$$

#### 向量形式

**Hypothesis**
$$
h = g(X\Theta)
\tag{1-1'}
$$

**Cost Function**
$$
J(\Theta)=\frac{1}{m} \cdot (-y^T\log(h)-(1-y^T)\log(1-h))
\tag{1-2'}
$$

**Gradient Descent**
$Repeat \; \lbrace$
$$
\Theta:=\Theta-\alpha \frac{1}{m}X^T(g(X\Theta)-\overrightarrow{y})
\tag{1-3'}
$$
$\rbrace$

### 应用

*该实例来源于 coursera machine learning programming exercise2.*

**目的**
用逻辑回归根据学生的考试成绩来判断学生是否可以入学。

**DataSet**
数据集的部分内容如下，完整数据集在[**这里**](https://raw.githubusercontent.com/seyvoue/coursera-ml/master/machine-learning-ex2/ex2/ex2data1.txt)

| Exam1 Score | Exam2 Score | Admitted/NotAdmitted |
| :-: | :-: | :-: |
| 34.62365962451697 | 78.0246928153624 | 0 |
| 30.28671076822607 | 43.89499752400101 | 0 |
| 35.84740876993872 | 72.90219802708364 | 0 |
| 60.18259938620976 | 86.30855209546826 | 1 |
| ... | ... | ... |

#### 完整代码

完整代码可在[**这里**](https://github.com/seyvoue/coursera-ml/tree/master/machine-learning-ex2/ex2)下载。

下面只列出其中的主程序：

```matlab
%% Machine Learning Online Class - Exercise 2: Logistic Regression
%% Load Data
%  The first two columns contains the exam scores and the third column
%  contains the label.

data = load('ex2data1.txt');
X = data(:, [1, 2]); y = data(:, 3);

%% ==================== Part 1: Plotting ====================
%  We start the exercise by first plotting the data to understand the 
%  the problem we are working with.

fprintf(['Plotting data with + indicating (y = 1) examples and o ' ...
         'indicating (y = 0) examples.\n']);

plotData(X, y);

% Put some labels 
hold on;
% Labels and Legend
xlabel('Exam 1 score')
ylabel('Exam 2 score')

% Specified in plot orde
legend('Admitted', 'Not admitted');
hold off;

fprintf('\nProgram paused. Press enter to continue.\n');
pause;


%% ============ Part 2: Compute Cost and Gradient ============
%  In this part of the exercise, you will implement the cost and gradient
%  for logistic regression. You neeed to complete the code in 
%  costFunction.m

%  Setup the data matrix appropriately, and add ones for the intercept term
[m, n] = size(X);

% Add intercept term to x and X_test
X = [ones(m, 1) X];

% Initialize fitting parameters
initial_theta = zeros(n + 1, 1);

% Compute and display initial cost and gradient
[cost, grad] = costFunction(initial_theta, X, y);

fprintf('Cost at initial theta (zeros): %f\n', cost);
fprintf('Expected cost (approx): 0.693\n');
fprintf('Gradient at initial theta (zeros): \n');
fprintf(' %f \n', grad);
fprintf('Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n');

% Compute and display cost and gradient with non-zero theta
test_theta = [-24; 0.2; 0.2];
[cost, grad] = costFunction(test_theta, X, y);

fprintf('\nCost at test theta: %f\n', cost);
fprintf('Expected cost (approx): 0.218\n');
fprintf('Gradient at test theta: \n');
fprintf(' %f \n', grad);
fprintf('Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n');

fprintf('\nProgram paused. Press enter to continue.\n');
pause;


%% ============= Part 3: Optimizing using fminunc  =============
%  In this exercise, you will use a built-in function (fminunc) to find the
%  optimal parameters theta.

%  Set options for fminunc
options = optimset('GradObj', 'on', 'MaxIter', 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
[theta, cost] = ...
	fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);

% Print theta to screen
fprintf('Cost at theta found by fminunc: %f\n', cost);
fprintf('Expected cost (approx): 0.203\n');
fprintf('theta: \n');
fprintf(' %f \n', theta);
fprintf('Expected theta (approx):\n');
fprintf(' -25.161\n 0.206\n 0.201\n');

% Plot Boundary
plotDecisionBoundary(theta, X, y);

% Put some labels 
hold on;
% Labels and Legend
xlabel('Exam 1 score')
ylabel('Exam 2 score')

% Specified in plot order
legend('Admitted', 'Not admitted')
hold off;

fprintf('\nProgram paused. Press enter to continue.\n');
pause;

%% ============== Part 4: Predict and Accuracies ==============
%  After learning the parameters, you'll like to use it to predict the outcomes
%  on unseen data. In this part, you will use the logistic regression model
%  to predict the probability that a student with score 45 on exam 1 and 
%  score 85 on exam 2 will be admitted.
%
%  Furthermore, you will compute the training and test set accuracies of 
%  our model.
%
%  Your task is to complete the code in predict.m

%  Predict probability for a student with score 45 on exam 1 
%  and score 85 on exam 2 

prob = sigmoid([1 45 85] * theta);
fprintf(['For a student with scores 45 and 85, we predict an admission ' ...
         'probability of %f\n'], prob);
fprintf('Expected value: 0.775 +/- 0.002\n\n');

% Compute accuracy on our training set
p = predict(theta, X);

fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);
fprintf('Expected accuracy (approx): 89.0\n');
fprintf('\n');
```

#### 代码分步讲解

> step1: 绘制散点图，观察训练集的数据分布
> step2: 计算 Cost Function $J(\Theta)$，以及梯度$\nabla J(\Theta)$
> step3: 调用 Octave/Matalb's fminunc function 找到 $J(\Theta)$ 的最优解 $\Theta$
> step4: 绘制 Decision Boundary
> step5: 模型的评估

- **step1: 绘制散点图，观察训练集的数据分布。**

```matlab
data = load('ex2data1.txt');
X = data(:, [1, 2]); y = data(:, 3);
plotData(X, y);
hold on;
xlabel('Exam 1 score')
ylabel('Exam 2 score')
legend('Admitted', 'Not admitted');
hold off;
```

> 其中`plotData(X,y)`函数的代码如下：

```matlab
function plotData(X, y)
%PLOTDATA Plots the data points X and y into a new figure 
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.

% Create New Figure
figure; hold on;

% Find indices of Positive and Negative
pos = find(y==1);
neg = find(y==0);

%Plot
plot(X(pos, 1), X(pos, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize',7);

hold off;

end
```

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-07-134449.jpg)

- **step2: 计算 Cost Function $J(\Theta)$，以及梯度$\nabla J(\Theta)$**

```matlab
%  Setup the data matrix appropriately, and add ones for the intercept term
[m, n] = size(X);
% Add intercept term to x and X_test
X = [ones(m, 1) X];
% Initialize fitting parameters
initial_theta = zeros(n + 1, 1);
% Compute and display initial cost and gradient
[cost, grad] = costFunction(initial_theta, X, y);
```

> 其中`costFunction(theta, X, y)`的代码如下：

```matlab
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples
J = 0;
grad = zeros(size(theta));

% Instructions: Compute the cost of a particular choice of theta.
J = ( y' * log(sigmoid(X*theta)) + (ones(size(y))-y)' * log(ones(size(X*theta))-sigmoid(X*theta)) )/(-m);
grad = ( X'*(sigmoid(X*theta)-y) )/m;

end
```

> 其中`sigmoid(z)`的代码如下：

```matlab
function g = sigmoid(z)
%SIGMOID Compute sigmoid function
%   g = SIGMOID(z) computes the sigmoid of z.

g = 1 ./ (ones(size(z)) + exp(-z));

end
```
- step3: 调用 Octave/Matalb's fminunc function 找到 $J(\Theta)$ 的最优解 $\Theta$

在线性回归中，我们通过自己编写 matlab 代码,如: [@线性回归](mweblib://15038392800368)。
本文我们通过使用 Octave/Matalb 中内嵌的 fminunc function，去求得最优解 $\Theta$，而不是自己使用 Gradient descent 在 for 循环求导来计算 $\Theta$。

```matlab
%  use a built-in function (fminunc) to find the
%  optimal parameters theta.

%  Set options for fminunc
options = optimset('GradObj', 'on', 'MaxIter', 400);

%  Run fminunc to obtain the optimal theta
%  This function will return theta and the cost 
[theta, cost] = ...
	fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
```

- step4: 绘制 Decision Boundary。

```matlab
% Plot Boundary
plotDecisionBoundary(theta, X, y);

% Put some labels 
hold on;
% Labels and Legend
xlabel('Exam 1 score')
ylabel('Exam 2 score')

% Specified in plot order
legend('Admitted', 'Not admitted')
hold off;
```

> 其中的`plotDecisionBoundary(theta, X, y)` 函数实现代码如下：

```matlab
function plotDecisionBoundary(theta, X, y)
%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with
%the decision boundary defined by theta
%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the 
%   positive examples and o for the negative examples. X is assumed to be 
%   a either 
%   1) Mx3 matrix, where the first column is an all-ones column for the 
%      intercept.
%   2) MxN, N>3 matrix, where the first column is all-ones

% Plot Data
plotData(X(:,2:3), y);
hold on

if size(X, 2) <= 3
    % Only need 2 points to define a line, so choose two endpoints
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];

    % Calculate the decision boundary line
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    
    % Legend, specific for the exercise
    legend('Admitted', 'Not admitted', 'Decision Boundary')
    axis([30, 100, 30, 100])
else
    % Here is the grid range
    u = linspace(-1, 1.5, 50);
    v = linspace(-1, 1.5, 50);

    z = zeros(length(u), length(v));
    % Evaluate z = theta*x over the grid
    for i = 1:length(u)
        for j = 1:length(v)
            z(i,j) = mapFeature(u(i), v(j))*theta;
        end
    end
    z = z'; % important to transpose z before calling contour

    % Plot z = 0
    % Notice you need to specify the range [0, 0]
    contour(u, v, z, [0, 0], 'LineWidth', 2)
end
hold off

end
```

> 其中的`mapFeature(X1, X2)`函数的实现代码如下：

```matlab
function out = mapFeature(X1, X2)
% MAPFEATURE Feature mapping function to polynomial features
%
%   MAPFEATURE(X1, X2) maps the two input features
%   to quadratic features used in the regularization exercise.
%
%   Returns a new feature array with more features, comprising of 
%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
%
%   Inputs X1, X2 must be the same size
%

degree = 6;
out = ones(size(X1(:,1)));
for i = 1:degree
    for j = 0:i
        out(:, end+1) = (X1.^(i-j)).*(X2.^j);
    end
end

end
```

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-005309.jpg)

- step5: 模型的评估

求得的逻辑回归模型是好还是坏呢？预测效果怎么样？需要拿一组数据测试一下，测试代码如下：

```matlab
prob = sigmoid([1 45 85] * theta); %这是一组测试数据，第一次考试成绩为45，第二次成绩为85
% Compute accuracy on our training set
p = predict(theta, X);% 调用predict函数测试模型
train_accuracy = mean(double(p == y)) * 100;
```

> 其中的 `predict(theta, X)` 函数的实现代码如下：

```matlab
function p = predict(theta, X)
%PREDICT Predict whether the label is 0 or 1 using learned logistic 
%regression parameters theta
%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)

m = size(X, 1); % Number of training examples

% You need to return the following variables correctly
p = zeros(m, 1);

p = X * theta >= 0;

end
```

原理如下：
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-010106.jpg)

---

## 多分类问题
逻辑回归如何处理多分类问题？

所谓 one-vs-all method 就是将二分类问题的方法应用到多分类问题中。比如: 我想分成$K$类。那么就将其中一类作为`positive`，另$k-1$合起来作为`negative`，这样进行$K$个$h_{\Theta}(X)$的参数优化；

$$
\begin{align*}& y \in \lbrace0, 1 ... n\rbrace \newline& h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline& h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline& \cdots \newline& h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline& \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}
\tag{2-1}
$$

每次得到的一个 $h_{\Theta}(X)$ 是指给定 $\Theta$ 和 $X$，它属于`positive`的类的概率。

给定一个输入向量 $X$，获得 $\max h_\Theta(X)$ 的类就是 $X$ 所分到的类。

所谓**多分类问题**，是指分类的结果为三类以上。比如，预测明天的天气结果为三类：晴(用 y==1 表示)、阴（用 y==2表示）、雨（用 y==3表示）

分类的思想，其实与逻辑回归分类(默认是指二分类，binary classification)很相似，对“晴天”进行分类时，将另外两类(阴天和下雨)视为一类：(非晴天)，这样，就把一个多分类问题转化成了二分类问题。示意图如下：(图中的圆圈 表示：不属于某一类的 所有其他类)

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-multiclass-classification.png)

### 应用

**目的**
用逻辑回归实现多分类问题：识别手写的阿拉伯数字(0~9)。

**DataSet**
完整数据集在[**这里**](https://github.com/seyvoue/coursera-ml/blob/master/machine-learning-ex3/ex3/ex3data1.mat)下载。
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-11-020444.jpg)
一共有 5000 个训练样本，每个样本是400维的向量（$20\times20$像素的 grayscale image），用矩阵 $X$ 保存。样本的结果(label of training set)保存在向量 $\overrightarrow{y}$ 中，$\overrightarrow{y}$ 是一个 $5000 \times 1$ 的列向量。

#### 完整代码
完整代码可在[**这里**](https://github.com/seyvoue/coursera-ml/tree/master/machine-learning-ex3/ex3)下载。

下面只列出其中的主程序：

```matlab
%% Machine Learning Online Class - Exercise 3 | Part 1: One-vs-all

%% Setup the parameters you will use for this part of the exercise
input_layer_size  = 400;  % 20x20 Input Images of Digits
num_labels = 10;          % 10 labels, from 1 to 10
                          % (note that we have mapped "0" to label 10)

%% =========== Part 1: Loading and Visualizing Data =============
%  We start the exercise by first loading and visualizing the dataset.
%  You will be working with a dataset that contains handwritten digits.
%

% Load Training Data
fprintf('Loading and Visualizing Data ...\n')

load('ex3data1.mat'); % training data stored in arrays X, y
m = size(X, 1);

% Randomly select 100 data points to display
rand_indices = randperm(m);
sel = X(rand_indices(1:100), :);

displayData(sel);

fprintf('Program paused. Press enter to continue.\n');
pause;

%% ============ Part 2a: Vectorize Logistic Regression ============
%  In this part of the exercise, you will reuse your logistic regression
%  code from the last exercise. You task here is to make sure that your
%  regularized logistic regression implementation is vectorized. After
%  that, you will implement one-vs-all classification for the handwritten
%  digit dataset.
%

% Test case for lrCostFunction
fprintf('\nTesting lrCostFunction() with regularization');

theta_t = [-2; -1; 1; 2];
X_t = [ones(5,1) reshape(1:15,5,3)/10];
y_t = ([1;0;1;0;1] >= 0.5);
lambda_t = 3;
[J grad] = lrCostFunction(theta_t, X_t, y_t, lambda_t);

fprintf('\nCost: %f\n', J);
fprintf('Expected cost: 2.534819\n');
fprintf('Gradients:\n');
fprintf(' %f \n', grad);
fprintf('Expected gradients:\n');
fprintf(' 0.146561\n -0.548558\n 0.724722\n 1.398003\n');

fprintf('Program paused. Press enter to continue.\n');
pause;
%% ============ Part 2b: One-vs-All Training ============
fprintf('\nTraining One-vs-All Logistic Regression...\n')

lambda = 0.1;
[all_theta] = oneVsAll(X, y, num_labels, lambda);

fprintf('Program paused. Press enter to continue.\n');
pause;


%% ================ Part 3: Predict for One-Vs-All ================

pred = predictOneVsAll(all_theta, X);

fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);
```

#### 代码分步讲解

> step1: 样本数据的可视化
> step2: 计算 Cost Function $J(\Theta)$，以及梯度$\nabla J(\Theta)$
> step3: 用 one-vs-all 方法实现多分类。调用 Octave/Matalb's `fminuncg` function 求出所有分类器的最优解 $\Theta$
> step4: 模型的评估

- **step1: 样本数据的可视化**

```matlab
load('ex3data1.mat'); % training data stored in arrays X, y
m = size(X, 1);

% Randomly select 100 data points to display
rand_indices = randperm(m);
sel = X(rand_indices(1:100), :);
displayData(sel);
```

> 其中 `displayData(X, example_width)` 代码如下：

```matlab
function [h, display_array] = displayData(X, example_width)
%DISPLAYDATA Display 2D data in a nice grid
%   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data
%   stored in X in a nice grid. It returns the figure handle h and the 
%   displayed array if requested.

% Set example_width automatically if not passed in
if ~exist('example_width', 'var') || isempty(example_width) 
	example_width = round(sqrt(size(X, 2)));
end

% Gray Image
colormap(gray);

% Compute rows, cols
[m n] = size(X);
example_height = (n / example_width);

% Compute number of items to display
display_rows = floor(sqrt(m));
display_cols = ceil(m / display_rows);

% Between images padding
pad = 1;

% Setup blank display
display_array = - ones(pad + display_rows * (example_height + pad), ...
                       pad + display_cols * (example_width + pad));

% Copy each example into a patch on the display array
curr_ex = 1;
for j = 1:display_rows
	for i = 1:display_cols
		if curr_ex > m, 
			break; 
		end
		% Copy the patch
		
		% Get the max value of the patch
		max_val = max(abs(X(curr_ex, :)));
		display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ...
		              pad + (i - 1) * (example_width + pad) + (1:example_width)) = ...
						reshape(X(curr_ex, :), example_height, example_width) / max_val;
		curr_ex = curr_ex + 1;
	end
	if curr_ex > m, 
		break; 
	end
end

% Display Image
h = imagesc(display_array, [-1 1]);

% Do not show axis
axis image off

drawnow;

end
```

*随机选择100个样本数据，使用Matlab可视化的结果如下：*
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-11-023038.jpg)

- **step2: 计算 Cost Function $J(\Theta)$，以及梯度$\nabla J(\Theta)$**

```matlab
function [J, grad] = lrCostFunction(theta, X, y, lambda)
%LRCOSTFUNCTION Compute cost and gradient for logistic regression with 
%regularization
%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

h = sigmoid(X*theta);

J = ( y'*log(h)+(ones(size(y))-y)'*log(ones(size(h))-h) )/(-m) + lambda/(2*m)*(theta(2:length(theta)))'*(theta(2:length(theta)));

grad = ( X' * ( h-y ) )/m + ( lambda / m ) * ( [0; ones( length(theta) - 1 , 1 )].*theta );

grad = grad(:);

end
```

- **用 one-vs-all 方法实现多分类。调用 Octave/Matalb's `fminuncg` function 求出所有分类器的最优解 $\Theta$**

```matlab
lambda = 0.1;
[all_theta] = oneVsAll(X, y, num_labels, lambda);
```

> 其中 `oneVsAll(X, y, num_labels, lambda)` 的代码如下：

```matlab
function [all_theta] = oneVsAll(X, y, num_labels, lambda)
%ONEVSALL trains multiple logistic regression classifiers and returns all
%the classifiers in a matrix all_theta, where the i-th row of all_theta 
%corresponds to the classifier for label i
%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels
%   logistic regression classifiers and returns each of these classifiers
%   in a matrix all_theta, where the i-th row of all_theta corresponds 
%   to the classifier for label i

% Some useful variables
m = size(X, 1);
n = size(X, 2);

% You need to return the following variables correctly 
all_theta = zeros(num_labels, n + 1);

% Add ones to the X data matrix
X = [ones(m, 1) X];

initial_theta = zeros(n+1,1);

options = optimset('GradObj','on','MaxIter',50);

for c = 1:num_labels %num_labels 为逻辑回归训练器的个数，num of logistic regression classifiers
	all_theta(c, :) = fmincg(@(t)(lrCostFunction(t, X, (y == c),lambda)), initial_theta,options );
end

end
```

- **step4: 模型的评估**
对于 $N$ 分类问题(N>=3)，就需要 $N$ 个假设函数(预测模型)，也即需要 $N$ 组模型参数 $\Theta$（$\Theta$一般是一个向量）

然后，对于每个样本实例，依次使用每个模型预测输出，选取输出值最大的那组模型所对应的预测结果作为最终结果。

因为模型的输出值，在sigmoid函数作用下，其实是一个概率值。

```matlab
pred = predictOneVsAll(all_theta, X);
tarin_accuracy = mean(double(pred == y)) * 100
```

> 其中 `predictOneVsAll(all_theta, X)` 代码如下：

```matlab
function p = predictOneVsAll(all_theta, X)
%PREDICT Predict the label for a trained one-vs-all classifier. The labels 
%are in the range 1..K, where K = size(all_theta, 1). 
%  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions
%  for each example in the matrix X. Note that X contains the examples in
%  rows. all_theta is a matrix where the i-th row is a trained logistic
%  regression theta vector for the i-th class. You should set p to a vector
%  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2
%  for 4 examples) 

m = size(X, 1);
num_labels = size(all_theta, 1);

% You need to return the following variables correctly 
p = zeros(size(X, 1), 1);

% Add ones to the X data matrix
X = [ones(m, 1) X];

[~,p] = max( X * all_theta',[],2); % 求矩阵(X*all_theta')每行的最大值，p 记录矩阵每行的最大值的索引

end
```

---

## 关于优化算法

Optimization Algorithms:

- Gradient Descent
- Conjugate Descent
- BFGS
- L-BFGS

后三种算法相比于 Gradient Descent：
Advantages:

- No need to manually pick $\alpha$
- often faster than Gradient Descent

Disadvantage:

- more complex

**Note:** 在线性回归和逻辑回归中，我们用到了两种优化算法 Gradient Descent(梯度下降法)和 Normal Equation(正规方程)。

---

## 参考链接

- [Stanford coursera Andrew Ng 机器学习课程编程作业（Exercise 2）及总结](http://www.cnblogs.com/hapjin/archive/2016/11/18/6078530.html)
- [ML Week 3 Lecture Notes -- Stanford coursera Andrew Ng](https://www.coursera.org/learn/machine-learning/resources/Zi29t)
- [stanford coursera 机器学习编程作业 exercise 3（逻辑回归实现多分类问题）](http://www.cnblogs.com/hapjin/p/6085278.html)