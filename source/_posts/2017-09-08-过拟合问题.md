---
title: '@过拟合问题'
comments: true
categories:  机器学习
abbrlink: bec21aa6
mathjax: true
date: 2017-09-08 13:33:41
updated: 2018-02-01 18:10:10
tags: ml
keywords:
description:
---

> overview: regularized linear and logistic regression, overfitting.

<!--more-->

## 概述

一般而言，当模型的特征 (feature variables) 非常多，而训练的样本数目 (training set) 又比较少的时候，训练得到的假设函数 (hypothesis function) 能够非常好地匹配训练集中的数据，此时的代价函数 (cost function) 几乎为0。下图中最右侧的那个模型 就是一个过拟合的模型。

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-715283-20161118191729545-602958584.png)

过拟合模型的弊端：尽管它几乎能完美匹配训练集中的每一个样本，但其不能很好地对未知的 (新样本实例)input instance 进行预测。通俗地说，就是过拟合模型的预测能力差。

> **正则化 (Regularization)**可以解决过拟合问题。

前面提到，正是因为 feature variable 非常多，导致 hypothesis function 的幂次很高，hypothesis function 变得很复杂(弯弯曲曲的)，从而能穿过每一个样本点(完美匹配每个样本)。如果添加一个"正则化项"，减少高幂次的特征变量的影响，hypothesis function 不就变得平滑了吗？

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-715283-20161118193423920-1528615002.png)

正如前面提到，梯度下降算法的目标是最小化 cost function，而现在把 $\theta_3$ 和 $\theta_4$ 的系数设置为1000，设得很大，求偏导数时，相应地得到的 $\theta_3$ 和 $\theta_4$ 就都约等于0了。

更一般地，我们对每一个 $\theta_j \;(j>=1)$ 进行正则化(注：不对 $\theta_0$ 正则化)，就得到了一个新的的代价函数 $\eqref{1-1}$：其中的 $\lambda$ 为正则化参数(regularization parameter)

![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-715283-20161118194003810-1506207063.png)

从上面的 $J(\theta)$ 可以看出：如果 $\lambda=0$ ，则表示没有使用正则化；如果 $\lambda$ 过大，使得模型的各个参数都变得很小，导致$h(x)=\theta_0$，从而造成欠拟合；如果 $\lambda$ 很小，则未充分起到正则化的效果。因此，$\lambda$ 的值要合适。

## 建模

### 线性回归的过拟合问题

**Hypothesis:**
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots
\tag{1-1}\label{1-1}
$$

**Cost Function**
$$
\begin{aligned}
&J(\theta) = \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2 \\
&\min_\theta J(\theta)
\end{aligned}
\tag{1-2}\label{1-2}
$$

**Gradient Descent**
$$
\begin{align*}
& \text{Repeat}\ \lbrace \newline
& \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
& \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
& \ \ \ \ \ \ \, \ = \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \newline
& \rbrace
\end{align*}
\tag{1-3}\label{1-3}
$$

另外$\eqref{1-3}$中，

$$
\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\Rightarrow
\text{向量形式为：}X^T(g(X\Theta)-\overrightarrow{y})
$$

**Normal Equation**
$$
\begin{align*}
& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline
& \text{where}\ \ L = \begin{bmatrix} 
0 & & & & \newline 
& 1 & & & \newline 
& & 1 & & \newline 
& & & \ddots & \newline 
& & & & 1 \newline
\end{bmatrix}
\end{align*}
\tag{1-4}\label{1-4}
$$

*Note: 在这里同时列出了 gradient descent 和 normal equation 两种求解 $J(\Theta)$ 最优解 $\Theta$ 的方法，具体可参考这篇文章([@线性回归](http://seyvoue.com/posts/91fff0b1/#more))*

### 逻辑回归的过拟合问题

**Hypothesis:**
$$
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_1x_2+\theta_5x_2^2+\theta_6x_1^3+\cdots)
$$

**Cost Function**
$$
J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
\tag{2-1}\label{2-1}
$$

> **Note:** $\sum_{j=1}^n \theta_j^2$中是不包含 $\theta_0$ 项的。 换句话说，$\overrightarrow{\theta}$是一个$(\theta_0..\theta_n)$的$n+1$维列向量，但公式中第二个$\sum$ 是从$\theta_1$开始的，不包含 $\theta_0$。

**Gradient Descent**
$$
\begin{align*}
& \text{Repeat}\ \lbrace \newline
& \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline
& \ \ \ \ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(
\theta) \newline
& \ \ \ \ \ \ \ \, = \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline
& \rbrace
\end{align*}
\tag{2-2}\label{2-2}
$$


## 应用

*该实例来源于 coursera machine learning programming exercise2.*

**目的**
用逻辑回归根据芯片的两次测试结果判断芯片是否合格。

**DataSet**
数据集的部分内容如下，完整数据集在[**这里**](https://raw.githubusercontent.com/seyvoue/coursera-ml/master/machine-learning-ex2/ex2/ex2data2.txt)

| microchip test1 | microchip test2 | Accepted/Rejected |
| :-: | :-: | :-: |
| 0.051267 | 0.69956 | 1 |
| -0.092742 | 0.68494 | 1 |
| 0.18376 | 0.93348 | 0 |
| 1.0709 | 0.10015 | 0 |
| ... | ... | ... |


### 完整代码

完整代码可在[**这里**](https://github.com/seyvoue/coursera-ml/tree/master/machine-learning-ex2/ex2)下载。

下面只列出其中的主程序：

```matlab
%% Machine Learning Online Class - Exercise 2: Logistic Regression Regularization
%% Load Data
%  The first two columns contains the X values and the third column
%  contains the label (y).

data = load('ex2data2.txt');
X = data(:, [1, 2]); y = data(:, 3);

plotData(X, y);

% Put some labels
hold on;

% Labels and Legend
xlabel('Microchip Test 1')
ylabel('Microchip Test 2')

% Specified in plot order
legend('y = 1', 'y = 0')
hold off;


%% =========== Part 1: Regularized Logistic Regression ============
%  In this part, you are given a dataset with data points that are not
%  linearly separable. However, you would still like to use logistic
%  regression to classify the data points.
%
%  To do so, you introduce more features to use -- in particular, you add
%  polynomial features to our data matrix (similar to polynomial
%  regression).
%

% Add Polynomial Features

% Note that mapFeature also adds a column of ones for us, so the intercept
% term is handled
X = mapFeature(X(:,1), X(:,2));

% Initialize fitting parameters
initial_theta = zeros(size(X, 2), 1);

% Set regularization parameter lambda to 1
lambda = 1;

% Compute and display initial cost and gradient for regularized logistic
% regression
[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);

fprintf('Cost at initial theta (zeros): %f\n', cost);
fprintf('Expected cost (approx): 0.693\n');
fprintf('Gradient at initial theta (zeros) - first five values only:\n');
fprintf(' %f \n', grad(1:5));
fprintf('Expected gradients (approx) - first five values only:\n');
fprintf(' 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\n');

fprintf('\nProgram paused. Press enter to continue.\n');
pause;

% Compute and display cost and gradient
% with all-ones theta and lambda = 10
test_theta = ones(size(X,2),1);
[cost, grad] = costFunctionReg(test_theta, X, y, 10);

fprintf('\nCost at test theta (with lambda = 10): %f\n', cost);
fprintf('Expected cost (approx): 3.16\n');
fprintf('Gradient at test theta - first five values only:\n');
fprintf(' %f \n', grad(1:5));
fprintf('Expected gradients (approx) - first five values only:\n');
fprintf(' 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\n');

fprintf('\nProgram paused. Press enter to continue.\n');
pause;

%% ============= Part 2: Regularization and Accuracies =============
%  Optional Exercise:
%  In this part, you will get to try different values of lambda and
%  see how regularization affects the decision coundart
%
%  Try the following values of lambda (0, 1, 10, 100).
%
%  How does the decision boundary change when you vary lambda? How does
%  the training set accuracy vary?
%

% Initialize fitting parameters
initial_theta = zeros(size(X, 2), 1);

% Set regularization parameter lambda to 1 (you should vary this)
lambda = 1;

% Set Options
options = optimset('GradObj', 'on', 'MaxIter', 400);

% Optimize
[theta, J, exit_flag] = ...
	fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);

% Plot Boundary
plotDecisionBoundary(theta, X, y);
hold on;
title(sprintf('lambda = %g', lambda))

% Labels and Legend
xlabel('Microchip Test 1')
ylabel('Microchip Test 2')

legend('y = 1', 'y = 0', 'Decision boundary')
hold off;

% Compute accuracy on our training set
p = predict(theta, X);

fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);
fprintf('Expected accuracy (with lambda = 1): 83.1 (approx)\n');
```

### 代码分步讲解

> step1: 绘制散点图，观察训练集的数据分布。
> step2: mapFeature。对特征$X$进行预处理，增加 feature 的个数
> step3: 计算 cost function $J(\Theta)$ 以及 $\nabla J(\Theta)$
> step4: 调用 Octave/Matalb's fminunc function 找到 > $J(\Theta)$ 的最优解$\Theta$
> step5: 绘制 Decision Boundary。
> step6: 模型评估

- **step1: 绘制散点图，观察训练集的数据分布。**

```matlab
data = load('ex2data2.txt');
X = data(:, [1, 2]); y = data(:, 3);
plotData(X, y);

hold on;
xlabel('Microchip Test 1')
ylabel('Microchip Test 2')
legend('y = 1', 'y = 0')
hold off;
```

> 其中`plotData(X,y)`函数的代码如下：

```matlab
function plotData(X, y)
%PLOTDATA Plots the data points X and y into a new figure 
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.

% Create New Figure
figure; hold on;

% Find indices of Positive and Negative
pos = find(y==1);
neg = find(y==0);

%Plot
plot(X(pos, 1), X(pos, 2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize',7);

hold off;

end
```
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-084558.jpg)

- **step2: mapFeature。对特征$X$进行预处理，增加 feature 的个数**

从上图可以猜测出，分类器是非线性的，$\theta_0+\theta_1x_1+\theta_2x_2$ 已无法满足要求，需要对 feature 做些处理，这里将特征的最高维度设为6维。

$\text{origin feature}:X=\begin{bmatrix} 1 \newline x_1 \newline x_2 \end{bmatrix} \quad \Rightarrow \text{after map feature}:X=\begin{bmatrix} 1 \newline x_1 \newline x_2 \newline x_1^2 \newline x_1x_2 \newline x_2^2 \newline x_1^3 \newline \cdots \newline x_1x_2^5 \newline x_2^6 \end{bmatrix}$

```matlab
X = mapFeature(X(:,1), X(:,2));
```

> 其中的 `mapFeature(X1, X2)` 函数的实现代码如下：

```matlab
function out = mapFeature(X1, X2)
% MAPFEATURE Feature mapping function to polynomial features
%
%   MAPFEATURE(X1, X2) maps the two input features
%   to quadratic features used in the regularization exercise.
%
%   Returns a new feature array with more features, comprising of 
%   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
%
%   Inputs X1, X2 must be the same size
%

degree = 6;
out = ones(size(X1(:,1)));
for i = 1:degree
    for j = 0:i
        out(:, end+1) = (X1.^(i-j)).*(X2.^j);
    end
end

end
```

- **step3: 计算 cost function $J(\Theta)$ 以及 $\nabla J(\Theta)$**

> **note:**此处的$J(\Theta)$以及$\nabla J(\Theta)$是带有正规化项的，即使用的是公式$\eqref{1-2}$

```matlab
% Initialize fitting parameters
initial_theta = zeros(size(X, 2), 1);

% Set regularization parameter lambda to 1
lambda = 1;

% Compute and display initial cost and gradient for regularized logistic
% regression
[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);
```

> 其中 `costFunctionReg(theta, X, y, lambda)` 函数的实现代码如下：

```matlab
function [J, grad] = costFunctionReg(theta, X, y, lambda)
%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

h = sigmoid(X*theta);
J = ( y'*log(h)+(ones(size(y))-y)'*log(ones(size(h))-h) )/(-m) + lambda/(2*m)*(theta(2:length(theta)))'*(theta(2:length(theta)));
grad = ( X' * ( h-y ) )/m + ( lambda / m ) * ( [0; ones( length(theta) - 1 , 1 )].*theta );

end
```

> 其中 `sigmoid(z)` 函数的代码如下：

```matlab
function g = sigmoid(z)
%SIGMOID Compute sigmoid function
%   g = SIGMOID(z) computes the sigmoid of z.

% You need to return the following variables correctly 
g = zeros(size(z));

g = 1 ./ (ones(size(z)) + exp(-z));

end
```

- **step4: 调用 Octave/Matalb's fminunc function 找到 $J(\Theta)$ 的最优解$\Theta$**

```matlab
% Initialize fitting parameters
initial_theta = zeros(size(X, 2), 1);

% Set regularization parameter lambda to 1 (you should vary this)
lambda = 1;

% Set Options
options = optimset('GradObj', 'on', 'MaxIter', 400);

% Optimize
[theta, J, exit_flag] = ...
	fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);
```

- **step5: 绘制 Decision Boundary。**

```matlab
plotDecisionBoundary(theta, X, y);
hold on;
title(sprintf('lambda = %g', lambda))

% Labels and Legend
xlabel('Microchip Test 1')
ylabel('Microchip Test 2')

legend('y = 1', 'y = 0', 'Decision boundary')
hold off;
```

> 其中 `plotDecisionBoundary(theta, X, y)` 代码如下：

```matlab
function plotDecisionBoundary(theta, X, y)
%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with
%the decision boundary defined by theta
%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the 
%   positive examples and o for the negative examples. X is assumed to be 
%   a either 
%   1) Mx3 matrix, where the first column is an all-ones column for the 
%      intercept.
%   2) MxN, N>3 matrix, where the first column is all-ones

% Plot Data
plotData(X(:,2:3), y);
hold on

if size(X, 2) <= 3
    % Only need 2 points to define a line, so choose two endpoints
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];

    % Calculate the decision boundary line
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    
    % Legend, specific for the exercise
    legend('Admitted', 'Not admitted', 'Decision Boundary')
    axis([30, 100, 30, 100])
else
    % Here is the grid range
    u = linspace(-1, 1.5, 50);
    v = linspace(-1, 1.5, 50);

    z = zeros(length(u), length(v));
    % Evaluate z = theta*x over the grid
    for i = 1:length(u)
        for j = 1:length(v)
            z(i,j) = mapFeature(u(i), v(j))*theta;
        end
    end
    z = z'; % important to transpose z before calling contour

    % Plot z = 0
    % Notice you need to specify the range [0, 0]
    contour(u, v, z, [0, 0], 'LineWidth', 2)
end
hold off

end
```
$\lambda=0$时，训练出来的模型（hypothesis function）如下：Train Accuracy: 86.440678
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-124454.jpg)
$\lambda=1$时，训练出来的模型（hypothesis function）如下：Train Accuracy: 83.050847
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-084626.jpg)
$\lambda=10$时，训练出来的模型（hypothesis function）如下：Train Accuracy: 74.576271
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-131130.jpg)
$\lambda=100$时，训练出来的模型（hypothesis function）如下：Train Accuracy: 61.016949
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-124524.jpg)

可以看出当 $\lambda=100$ 时，已经出现了欠拟合；当$\lambda=0$时，模型与样本的拟合度最好，准确度 train accuracy 最高，由于评估时使用的是训练集(即原样本，不是新样本)，所以模型的准确度 train accuracy 比 $\lambda=1$ 时还要高，但若使用样本评估模型，其准确度将会有所降低，因为从 decision boundary 可以看出模型已经过拟合了。

- **step6: 模型评估**

求得的逻辑回归模型是好还是坏呢？预测效果怎么样？需要拿一组数据测试一下，测试代码如下：

```matlab
% Compute accuracy on our training set
p = predict(theta, X);% 调用predict函数测试模型
train_accuracy = mean(double(p == y)) * 100;
```

> 其中的 `predict(theta, X)` 函数的实现代码如下：

```matlab
function p = predict(theta, X)
%PREDICT Predict whether the label is 0 or 1 using learned logistic 
%regression parameters theta
%   p = PREDICT(theta, X) computes the predictions for X using a 
%   threshold at 0.5 (i.e., if sigmoid(theta'*x) >= 0.5, predict 1)

m = size(X, 1); % Number of training examples

% You need to return the following variables correctly
p = zeros(m, 1);

p = X * theta >= 0;

end
```

原理如下：
![](http://ipic-markdown.oss-cn-shanghai.aliyuncs.com/blog/2017-09-08-010106.jpg)





